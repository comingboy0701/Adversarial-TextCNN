Loading data...
Vocab size: 4762
<bound method Module.parameters of Model(
  (embedding): Embedding(4762, 300, padding_idx=4761)
  (convs): ModuleList(
    (0): Conv2d(1, 256, kernel_size=(2, 300), stride=(1, 1))
    (1): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))
    (2): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))
  )
  (dropout): Dropout(p=0.5, inplace=False)
  (fc): Linear(in_features=768, out_features=10, bias=True)
)>
Epoch [1/20]
Iter:      0,  Train Loss:   2.3,  Train Acc: 11.72%,  Val Loss:   2.2,  Val Acc: 23.70%,  Time: 0:00:00 *
Iter:    100,  Train Loss:  0.88,  Train Acc: 70.31%,  Val Loss:  0.69,  Val Acc: 78.77%,  Time: 0:00:02 *
Iter:    200,  Train Loss:   1.1,  Train Acc: 67.97%,  Val Loss:  0.58,  Val Acc: 82.30%,  Time: 0:00:05 *
Iter:    300,  Train Loss:  0.61,  Train Acc: 79.69%,  Val Loss:  0.53,  Val Acc: 83.67%,  Time: 0:00:07 *
Iter:    400,  Train Loss:   1.0,  Train Acc: 69.53%,  Val Loss:  0.52,  Val Acc: 84.50%,  Time: 0:00:09 *
Iter:    500,  Train Loss:  0.59,  Train Acc: 81.25%,  Val Loss:  0.48,  Val Acc: 85.05%,  Time: 0:00:11 *
Iter:    600,  Train Loss:  0.65,  Train Acc: 78.91%,  Val Loss:  0.46,  Val Acc: 85.90%,  Time: 0:00:13 *
Iter:    700,  Train Loss:  0.72,  Train Acc: 78.12%,  Val Loss:  0.46,  Val Acc: 85.72%,  Time: 0:00:15 *
Iter:    800,  Train Loss:  0.56,  Train Acc: 84.38%,  Val Loss:  0.45,  Val Acc: 86.06%,  Time: 0:00:17 *
Iter:    900,  Train Loss:  0.71,  Train Acc: 81.25%,  Val Loss:  0.43,  Val Acc: 86.72%,  Time: 0:00:20 *
Iter:   1000,  Train Loss:  0.49,  Train Acc: 82.03%,  Val Loss:  0.46,  Val Acc: 85.75%,  Time: 0:00:22 
Iter:   1100,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.43,  Val Acc: 86.43%,  Time: 0:00:24 
Iter:   1200,  Train Loss:  0.48,  Train Acc: 84.38%,  Val Loss:  0.42,  Val Acc: 86.82%,  Time: 0:00:26 *
Iter:   1300,  Train Loss:   0.7,  Train Acc: 82.81%,  Val Loss:  0.42,  Val Acc: 86.67%,  Time: 0:00:28 *
Iter:   1400,  Train Loss:  0.66,  Train Acc: 84.38%,  Val Loss:  0.41,  Val Acc: 87.33%,  Time: 0:00:30 *
Epoch [2/20]
Iter:   1500,  Train Loss:  0.54,  Train Acc: 80.47%,  Val Loss:   0.4,  Val Acc: 87.55%,  Time: 0:00:32 *
Iter:   1600,  Train Loss:  0.34,  Train Acc: 89.06%,  Val Loss:   0.4,  Val Acc: 87.55%,  Time: 0:00:35 *
Iter:   1700,  Train Loss:  0.52,  Train Acc: 86.72%,  Val Loss:   0.4,  Val Acc: 87.80%,  Time: 0:00:37 
Iter:   1800,  Train Loss:  0.38,  Train Acc: 88.28%,  Val Loss:  0.41,  Val Acc: 87.44%,  Time: 0:00:39 
Iter:   1900,  Train Loss:  0.53,  Train Acc: 85.94%,  Val Loss:   0.4,  Val Acc: 87.79%,  Time: 0:00:41 *
Iter:   2000,  Train Loss:  0.52,  Train Acc: 80.47%,  Val Loss:  0.39,  Val Acc: 87.89%,  Time: 0:00:43 *
Iter:   2100,  Train Loss:  0.46,  Train Acc: 85.94%,  Val Loss:  0.38,  Val Acc: 88.64%,  Time: 0:00:45 *
Iter:   2200,  Train Loss:  0.39,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.34%,  Time: 0:00:47 
Iter:   2300,  Train Loss:  0.39,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.51%,  Time: 0:00:49 *
Iter:   2400,  Train Loss:  0.37,  Train Acc: 89.84%,  Val Loss:  0.39,  Val Acc: 88.30%,  Time: 0:00:52 
Iter:   2500,  Train Loss:  0.23,  Train Acc: 92.19%,  Val Loss:  0.38,  Val Acc: 88.70%,  Time: 0:00:54 *
Iter:   2600,  Train Loss:  0.49,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.61%,  Time: 0:00:56 
Iter:   2700,  Train Loss:  0.31,  Train Acc: 89.06%,  Val Loss:  0.38,  Val Acc: 88.61%,  Time: 0:00:58 
Iter:   2800,  Train Loss:  0.56,  Train Acc: 82.03%,  Val Loss:  0.37,  Val Acc: 88.09%,  Time: 0:01:00 *
Epoch [3/20]
Iter:   2900,  Train Loss:  0.39,  Train Acc: 86.72%,  Val Loss:  0.36,  Val Acc: 88.96%,  Time: 0:01:02 *
Iter:   3000,  Train Loss:  0.38,  Train Acc: 84.38%,  Val Loss:  0.38,  Val Acc: 88.36%,  Time: 0:01:04 
Iter:   3100,  Train Loss:  0.48,  Train Acc: 85.16%,  Val Loss:  0.38,  Val Acc: 88.28%,  Time: 0:01:06 
Iter:   3200,  Train Loss:  0.47,  Train Acc: 91.41%,  Val Loss:  0.38,  Val Acc: 88.68%,  Time: 0:01:09 
Iter:   3300,  Train Loss:  0.45,  Train Acc: 84.38%,  Val Loss:  0.37,  Val Acc: 88.75%,  Time: 0:01:11 
Iter:   3400,  Train Loss:  0.55,  Train Acc: 83.59%,  Val Loss:  0.36,  Val Acc: 89.15%,  Time: 0:01:13 
Iter:   3500,  Train Loss:  0.24,  Train Acc: 91.41%,  Val Loss:  0.37,  Val Acc: 88.90%,  Time: 0:01:15 
Iter:   3600,  Train Loss:  0.28,  Train Acc: 92.97%,  Val Loss:  0.38,  Val Acc: 88.74%,  Time: 0:01:17 
Iter:   3700,  Train Loss:  0.45,  Train Acc: 85.94%,  Val Loss:  0.37,  Val Acc: 88.87%,  Time: 0:01:19 
Iter:   3800,  Train Loss:  0.52,  Train Acc: 82.03%,  Val Loss:  0.37,  Val Acc: 88.90%,  Time: 0:01:21 
Iter:   3900,  Train Loss:  0.39,  Train Acc: 85.94%,  Val Loss:  0.39,  Val Acc: 88.18%,  Time: 0:01:23 
No optimization for a long time, auto-stopping...
Test Loss:  0.34,  Test Acc: 89.18%
Precision, Recall and F1-Score...
               precision    recall  f1-score   support

      finance     0.9176    0.8800    0.8984      1000
       realty     0.9025    0.9160    0.9092      1000
       stocks     0.8447    0.8270    0.8358      1000
    education     0.9589    0.9330    0.9458      1000
      science     0.8137    0.8650    0.8386      1000
      society     0.8790    0.8790    0.8790      1000
     politics     0.8773    0.8650    0.8711      1000
       sports     0.9172    0.9640    0.9400      1000
         game     0.9099    0.8890    0.8993      1000
entertainment     0.9027    0.9000    0.9014      1000

     accuracy                         0.8918     10000
    macro avg     0.8924    0.8918    0.8919     10000
 weighted avg     0.8924    0.8918    0.8919     10000

Confusion Matrix...
[[880  20  50   1  15   8  10  11   3   2]
 [ 11 916  22   2   7  16   8   3   5  10]
 [ 49  30 827   2  48   3  29   4   5   3]
 [  1   4   5 933   7  16  13   5   5  11]
 [  1   6  26   5 865  15  18   8  37  19]
 [  5  19   4  15  20 879  33   4   4  17]
 [  9   5  31   7  23  37 865  10   3  10]
 [  1   2   3   1   7   6   3 964   2  11]
 [  1   5   8   3  55   7   3  15 889  14]
 [  1   8   3   4  16  13   4  27  24 900]]
Test time usage: 0:00:00
All time usage: 0:01:30
